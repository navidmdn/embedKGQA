{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = './data'\n",
    "KB_PATH = './data/kb.txt'\n",
    "\n",
    "TEST_SPLIT = 0.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'directed_by': 0, 'written_by': 1, 'starred_actors': 2, 'release_year': 3, 'in_language': 4, 'has_tags': 5, 'has_genre': 6, 'has_imdb_votes': 7, 'has_imdb_rating': 8}\n"
     ]
    }
   ],
   "source": [
    "class KBManager:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.entity_map = {}\n",
    "        self.relation_map = {}\n",
    "        self._build_vocabs()\n",
    "        \n",
    "    @staticmethod\n",
    "    def extend_vocab(word, vocab):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    def _build_vocabs(self):\n",
    "        with open(self.data_dir, 'r') as f:\n",
    "            data = f.read()\n",
    "            lines = data.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                subj, rel, obj = line.split('|')\n",
    "                self.extend_vocab(subj, self.entity_map)\n",
    "                self.extend_vocab(obj, self.entity_map)\n",
    "                self.extend_vocab(rel, self.relation_map)\n",
    "    \n",
    "    \n",
    "    def load_er_vocab(self):\n",
    "        result = {}\n",
    "        with open(self.data_dir, 'r') as f:\n",
    "            data = f.read()\n",
    "            lines = data.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                subj, rel, obj = line.split('|')\n",
    "                subj_idx = self.entity_map[subj]\n",
    "                rel_idx = self.relation_map[rel]\n",
    "                obj_idx = self.entity_map[obj]\n",
    "                \n",
    "                er_tuple = (subj_idx, rel_idx)\n",
    "                if er_tuple in result:\n",
    "                    result[er_tuple].append(obj_idx)\n",
    "                else:\n",
    "                    result[er_tuple] = [obj_idx]\n",
    "        return result\n",
    "                \n",
    "    \n",
    "\n",
    "kb_mgr = KBManager(KB_PATH)\n",
    "er_vocab = kb_mgr.load_er_vocab()\n",
    "\n",
    "print(kb_mgr.relation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68556 12099\n"
     ]
    }
   ],
   "source": [
    "raw_ds = list(kb_mgr.load_er_vocab().items())\n",
    "\n",
    "train_raw_ds, test_raw_ds = train_test_split(raw_ds, test_size=TEST_SPLIT, shuffle=True)\n",
    "\n",
    "print(len(train_raw_ds), len(test_raw_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 3],\n",
       "        [3, 4]]), array([[1., 1., 0.],\n",
       "        [0., 1., 1.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(dataset,  target_dim, batch_size=64):\n",
    "    batch_idx = 0\n",
    "    \n",
    "    remainder = len(dataset) % batch_size\n",
    "    if remainder != 0:\n",
    "        dataset = dataset[:-remainder]\n",
    "        \n",
    "    while batch_idx < len(dataset):\n",
    "        batch = dataset[batch_idx:batch_idx+batch_size, :]\n",
    "        Xs = np.array(list(zip(*batch[:,0])))\n",
    "        Ys = np.zeros((batch_size, target_dim))\n",
    "        for idx, targets in enumerate(batch[:,1]):\n",
    "            Ys[idx][targets] = 1\n",
    "        yield Xs, Ys\n",
    "        batch_idx += batch_size\n",
    "                    \n",
    "next(iter(get_batch(np.array([[(2,3), [0,1]], [(3,4), [1,2]]]), target_dim=3, batch_size=2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"kbg_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  22135808  \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      multiple                  4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  8         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 22,141,448\n",
      "Trainable params: 22,140,932\n",
      "Non-trainable params: 516\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "\n",
    "class KBGModel(keras.Model):\n",
    "    \n",
    "    def __init__(self, entity_dim, relation_dim, hidden_dim):\n",
    "        super(KBGModel, self).__init__()\n",
    "    \n",
    "        self.entity_dim = entity_dim\n",
    "        self.relation_dim = relation_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.entity_encoder = keras.layers.Embedding(\n",
    "            self.entity_dim,\n",
    "            self.hidden_dim,\n",
    "            embeddings_regularizer=keras.regularizers.l2(0.1)\n",
    "        )\n",
    "        \n",
    "        self.relation_encoder = keras.layers.Embedding(\n",
    "            self.relation_dim,\n",
    "            self.hidden_dim,\n",
    "            input_shape=(),\n",
    "        )\n",
    "        \n",
    "        self.head_bn = keras.layers.BatchNormalization()\n",
    "        self.head_drpout = keras.layers.Dropout(0.3)\n",
    "        self.rel_drpout = keras.layers.Dropout(0.4)\n",
    "        self.score_bn = keras.layers.BatchNormalization()\n",
    "        self.output_drpout = keras.layers.Dropout(0.5)\n",
    "        \n",
    "    def get_score(self, head, relation, entity_encoder):\n",
    "        \n",
    "        assert self.hidden_dim % 2 == 0\n",
    "        hidden_dim_slice = int(self.hidden_dim/2)\n",
    "\n",
    "        head_norm = self.head_bn(tf.reshape(head, (-1, hidden_dim_slice, 2)))\n",
    "        head_drp = self.head_drpout(head_norm)\n",
    "\n",
    "        head_drp = tf.reshape(head_drp, (-1, self.hidden_dim))\n",
    "\n",
    "        re_head = tf.slice(head_drp, [0, 0], [-1, hidden_dim_slice])\n",
    "        im_head = tf.slice(head_drp, [0, hidden_dim_slice], [-1, -1])\n",
    "\n",
    "        relation_drp = self.rel_drpout(tf.squeeze(relation))\n",
    "        re_relation = tf.slice(relation_drp, [0, 0], [-1, hidden_dim_slice])\n",
    "        im_relation = tf.slice(relation_drp, [0, hidden_dim_slice], [-1, -1])\n",
    "\n",
    "        re_tail = tf.slice(tf.squeeze(entity_encoder.weights), [0, 0], [-1, hidden_dim_slice])\n",
    "        im_tail = tf.slice(tf.squeeze(entity_encoder.weights), [0, hidden_dim_slice], [-1, -1])\n",
    "\n",
    "        re_score = re_head * re_relation - im_head * im_relation\n",
    "        im_score = re_head * im_relation + im_head * re_relation\n",
    "\n",
    "        score = tf.stack([re_score, im_score], axis=1)\n",
    "        score_bn = self.score_bn(score)\n",
    "        score_drp = self.output_drpout(score_bn)\n",
    "\n",
    "        score_drp = tf.reshape(score_drp, (-1, self.hidden_dim))\n",
    "        re_score = tf.slice(score_drp, [0, 0], [-1, hidden_dim_slice])\n",
    "        im_score = tf.slice(score_drp, [0, hidden_dim_slice], [-1, -1])\n",
    "\n",
    "        scores = tf.add(\n",
    "            tf.matmul(re_score, re_tail, transpose_b=True),\n",
    "            tf.matmul(im_score, im_tail, transpose_b=True)\n",
    "        )\n",
    "\n",
    "        return scores\n",
    "        \n",
    "    def call(self, subj_ids, rel_ids):\n",
    "        entity_embedding = self.entity_encoder(subj_ids)\n",
    "        rel_embedding = self.relation_encoder(rel_ids)\n",
    "        \n",
    "        scores = self.get_score(entity_embedding, rel_embedding, self.entity_encoder)\n",
    "        prediction = tf.sigmoid(scores)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "entity_dim = len(kb_mgr.entity_map)\n",
    "relation_dim = len(kb_mgr.relation_map)\n",
    "model = KBGModel(entity_dim, relation_dim, EMBEDDING_DIM)\n",
    "\n",
    "model(\n",
    "    np.array([425, 77]),\n",
    "    np.array([1,2]),\n",
    "    training=False\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss function and gradient methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = keras.metrics.Mean(name='validation_loss')\n",
    "\n",
    "@tf.function\n",
    "def train_step(subj_ids, rel_ids, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(\n",
    "            subj_ids,\n",
    "            rel_ids,\n",
    "            training=True\n",
    "        )\n",
    "        loss = loss_fn(y_true=targets, y_pred=predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    \n",
    "@tf.function\n",
    "def validation_step(subj_ids, rel_ids, targets):\n",
    "    predictions = model(\n",
    "        subj_ids,\n",
    "        rel_ids,\n",
    "        training=False\n",
    "    )\n",
    "    loss = loss_fn(y_true=targets, y_pred=predictions)\n",
    "    validation_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "metrics to calculate hits and evaluate training is in the source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_BATCH_SIZE = 2048\n",
    "TRAIN_LOG_STEP = 20\n",
    "\n",
    "train_ds = np.array(train_raw_ds)\n",
    "test_ds = np.array(test_raw_ds)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    iteration = 0\n",
    "    for x, y in get_batch(train_ds, entity_dim, batch_size=BATCH_SIZE):\n",
    "        subj_ids, rel_ids = list(zip(x))    \n",
    "        train_step(np.array(subj_ids), np.array(rel_ids), y)\n",
    "        \n",
    "        if not iteration % TRAIN_LOG_STEP:\n",
    "            print('training loss in iteration {}: {}'.format(iteration, train_loss.result()))\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "    for x, y in get_batch(test_ds, entity_dim, batch_size=VALIDATION_BATCH_SIZE):\n",
    "        subj_ids, rel_ids = list(zip(x))    \n",
    "        validation_step(np.array(subj_ids), np.array(rel_ids), y)\n",
    "    \n",
    "    print(\"epoch:{} validation_loss:{}\".format(epoch, validation_loss.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"kbg_model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     multiple                  22135808  \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     multiple                  4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc multiple                  8         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc multiple                  1024      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 22,141,448\n",
      "Trainable params: 22,140,932\n",
      "Non-trainable params: 516\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = KBGModel(entity_dim, relation_dim, EMBEDDING_DIM)\n",
    "model.load_weights('complex_graph_embedding/data/saved_models/complex')\n",
    "\n",
    "model(\n",
    "    np.array([425, 77]),\n",
    "    np.array([1,2]),\n",
    "    training=False\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 43234), dtype=float32, numpy=\n",
       "array([[1.2240209e-05, 1.8087108e-04, 7.7390405e-06, ..., 3.5553062e-06,\n",
       "        3.7217113e-05, 1.1254337e-05],\n",
       "       [6.3883745e-06, 6.3407322e-05, 7.9821511e-06, ..., 4.7250278e-06,\n",
       "        1.8054196e-05, 6.6062339e-06]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    np.array([425, 77]),\n",
    "    np.array([1,2]),\n",
    "    training=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
